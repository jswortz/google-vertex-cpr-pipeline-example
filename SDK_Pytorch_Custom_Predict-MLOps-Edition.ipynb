{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Pytorch_Custom_Predict.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/prediction/custom_prediction_routines/SDK_Pytorch_Custom_Predict.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI SDK to build a custom container that uses the Custom Prediction Routine model server to serve a PyTorch model on Vertex AI Predictions.\n",
    "\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This tutorial uses R.A. Fisher's Iris dataset, a small dataset that is popular for trying out machine learning techniques. Each instance has four numerical features, which are different measurements of a flower, and a target label that\n",
    "marks it as one of three types of iris: Iris setosa, Iris versicolour, or Iris virginica.\n",
    "\n",
    "This tutorial uses [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to:\n",
    "- Train a model that uses a flower's measurements as input to predict what type of iris it is.\n",
    "- Save the model.\n",
    "- Build a custom PyTorch serving container with custom preprocessing using the Custom Prediction Routine feature in the Vertex AI SDK.\n",
    "- Test the built container locally.\n",
    "- Upload and deploy custom container to Vertex Prediction.\n",
    "\n",
    "This tutorial focuses more on deploying this model with Vertex AI than on\n",
    "the design of the model itself.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* Docker\n",
    "* Git\n",
    "* Google Cloud SDK (gcloud)\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as NumPy, Scikit-learn, FastAPI, Uvicorn, and joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! pip3 install --upgrade --user -q google-cloud-aiplatform\n",
    "! pip3 install --upgrade --user -q google-cloud-storage\n",
    "! pip3 install --upgrade --user -q kfp\n",
    "! pip3 install --upgrade --user -q google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.3.0\n",
      "google_cloud_pipeline_components version: 2.4.1\n",
      "aiplatform SDK version: 1.35.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component).\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` or `%` as shell commands, and it interpolates Python variables with `$` or `{}` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  wortz-project-352116\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"wortz-project-352116\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure project and resource names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYxy9q6vVNIw"
   },
   "source": [
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AZ6rEERzVPdb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "EXPERIMENT_NAME = f'custom-pipe-pytorch-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE0Zo2mxVVMT"
   },
   "source": [
    "Configure GCP resource names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACT_DIR = \"pytorch-cpr-model-sdk\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"custom-container-prediction-sdk\"  # @param {type:\"string\"}\n",
    "IMAGE = \"pytorch-cpr-server-sdk\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"pytorch-cpr-model-sdk\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca1a915d641d"
   },
   "source": [
    "`REGION` - Used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Cloud\n",
    "Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#feature-availability). You may\n",
    "not use a Multi-Regional Storage bucket for prediction with Vertex AI.\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that will be pushed.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "To update your model artifacts without re-building the container, you must upload your model\n",
    "artifacts and any custom code to Cloud Storage.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"pytorch-example-jsw\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "MODEL_BLOB = \"models/custom/model.pt\"\n",
    "MODEL_URI = f\"{BUCKET_URI}/models/custom/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://pytorch-example-jsw/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'pytorch-example-jsw' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://pytorch-example-jsw//\n",
      "                                 gs://pytorch-example-jsw/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "### Set up directories and constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "Decide the directory to put your all custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "outputs": [],
   "source": [
    "SERVING_APP_DIR = \"serving_app\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cee1ac13b767"
   },
   "source": [
    "Decide the directory to put your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "104037d0dbf5"
   },
   "outputs": [],
   "source": [
    "# LOCAL_MODEL_ARTIFACTS_DIR = \"model_artifacts\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_DIR = \"trainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training constants\n",
    "APP_NAME = 'torch-cpr-train-example'\n",
    "# TRAIN_REPO_NAME=f'{APP_NAME}-train' # Name of repository in which we will store our custom training image\n",
    "TRAIN_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{APP_NAME}-train:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_IMAGE = \"pytorch-custom-prediction\"  # @param {type:\"string\"} \n",
    "SERVING_IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{SERVER_IMAGE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT_PATH = f'gs://{BUCKET_NAME}/pipeline_root'\n",
    "\n",
    "PIPELINE_JSON_SPEC_LOCAL = \"custom_pipeline_spec.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6e74556ea0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘serving_app’: File exists\n",
      "mkdir: cannot create directory ‘trainer’: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir $SERVING_APP_DIR\n",
    "# %mkdir $LOCAL_MODEL_ARTIFACTS_DIR\n",
    "%mkdir $TRAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "212b2935ea12"
   },
   "source": [
    "### Configure the artifact registry\n",
    "\n",
    "Configure Docker to access Artifact Registry. Then push your container image to your Artifact Registry repository after we build the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wSFXCj3LdluJ"
   },
   "outputs": [],
   "source": [
    "# !gcloud services list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABE9UpwSdluK"
   },
   "source": [
    "If `artifactregistry.googleapis.com` is not enabled in your project, enable the API before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qDhLoQMydluK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.services.enable) PERMISSION_DENIED: Permission denied to enable service [artifactregistry.googleapis.com]\n",
      "Help Token: AVzH8v1JABU_5AWsULciBElpAw0ajBAQ195xH0Lxh5wyhJhQtIuDQgQK3OPqVpmCvug_SHma4W1dAMyFcGXhxJKNqNmpjeAa2OZqOjswpG6IAys5\n",
      "- '@type': type.googleapis.com/google.rpc.PreconditionFailure\n",
      "  violations:\n",
      "  - subject: ?error_code=110002&service=serviceusage.googleapis.com&permission=serviceusage.services.enable&resource=wortz-project-352116\n",
      "    type: googleapis.com\n",
      "- '@type': type.googleapis.com/google.rpc.ErrorInfo\n",
      "  domain: serviceusage.googleapis.com\n",
      "  metadata:\n",
      "    permission: serviceusage.services.enable\n",
      "    resource: wortz-project-352116\n",
      "    service: serviceusage.googleapis.com\n",
      "  reason: AUTH_PERMISSION_DENIED\n"
     ]
    }
   ],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "09ffe2434e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create {REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "293437024749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train.py for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "950c429a65ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $TRAINER_DIR/train.py\n",
    "\n",
    "import argparse\n",
    "from urllib.request import urlretrieve\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    DATA_DIR = \".\"\n",
    "\n",
    "    LOCAL_DATA_FILE = f\"{DATA_DIR}/iris.csv\"\n",
    "\n",
    "    urlretrieve(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "        LOCAL_DATA_FILE,\n",
    "    )\n",
    "\n",
    "    ### Build a PyTorch NN Classifier\n",
    "    print(\"PyTorch Version: {}\".format(torch.__version__))\n",
    "\n",
    "\n",
    "    CLASS_VOCAB = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "\n",
    "    # Step 1. Load data\n",
    "    # In this step, we are going to:\n",
    "\n",
    "    # Load the data to Pandas Dataframe.\n",
    "    # Convert the class feature (species) from string to a numeric indicator.\n",
    "    # Split the Dataframe into input feature (xtrain) and target feature (ytrain).\n",
    "\n",
    "    datatrain = pd.read_csv(\n",
    "        LOCAL_DATA_FILE,\n",
    "        names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"],\n",
    "    )\n",
    "\n",
    "    # change string value to numeric\n",
    "    datatrain.loc[datatrain[\"species\"] == \"Iris-setosa\", \"species\"] = 0\n",
    "    datatrain.loc[datatrain[\"species\"] == \"Iris-versicolor\", \"species\"] = 1\n",
    "    datatrain.loc[datatrain[\"species\"] == \"Iris-virginica\", \"species\"] = 2\n",
    "    datatrain = datatrain.apply(pd.to_numeric)\n",
    "\n",
    "    # change dataframe to array\n",
    "    datatrain_array = datatrain.values\n",
    "\n",
    "    # split x and y (feature and target)\n",
    "    xtrain = datatrain_array[:, :4]\n",
    "    ytrain = datatrain_array[:, 4]\n",
    "\n",
    "    input_features = xtrain.shape[1]\n",
    "    num_classes = len(CLASS_VOCAB)\n",
    "\n",
    "    print(\"Records loaded: {}\".format(len(xtrain)))\n",
    "    print(\"Number of input features: {}\".format(input_features))\n",
    "    print(\"Number of classes: {}\".format(num_classes))\n",
    "\n",
    "\n",
    "    # Step 2. Set model parameters\n",
    "    # You can try different values for hidden_units or learning_rate.\n",
    "\n",
    "    HIDDEN_UNITS = 10\n",
    "    LEARNING_RATE = 0.1\n",
    "\n",
    "    # Step 3. Define the PyTorch NN model\n",
    "    # Here, we build a a neural network with one hidden layer, and a Softmax output layer for classification.\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_features, HIDDEN_UNITS),\n",
    "        torch.nn.Sigmoid(),\n",
    "        torch.nn.Linear(HIDDEN_UNITS, num_classes),\n",
    "        torch.nn.Softmax(),\n",
    "    )\n",
    "\n",
    "    loss_metric = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Step 4. Train the model\n",
    "    # We are going to train the model for num_epoch epochs.\n",
    "\n",
    "    NUM_EPOCHS = 10000\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        x = Variable(torch.Tensor(xtrain).float())\n",
    "        y = Variable(torch.Tensor(ytrain).long())\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_metric(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch) % 1000 == 0:\n",
    "            print(\n",
    "                \"Epoch [{}/{}] Loss: {}\".format(\n",
    "                    epoch + 1, NUM_EPOCHS, round(loss.item(), 3)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"Epoch [{}/{}] Loss: {}\".format(epoch + 1, NUM_EPOCHS, round(loss.item(), 3)))\n",
    "    \n",
    "    # Save the model to GCS\n",
    "\n",
    "    storage_client = storage.Client(args.model_bucket)\n",
    "    bucket = storage_client.bucket(args.model_bucket)\n",
    "    blob = bucket.blob(args.model_blob)\n",
    "    with blob.open(\"wb\", ignore_flush=True) as f:\n",
    "        torch.save(model, f)\n",
    "        \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Pytorch Iris model')\n",
    "    parser.add_argument('--project_id', type=str, help='GCP Project ID')\n",
    "    parser.add_argument('--model_bucket', type=str, help='Model Bucket Name')\n",
    "    parser.add_argument('--model_blob', type=str, help='Model Blob Path')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train(args) #will return a torch model artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2b675059515"
   },
   "source": [
    "## Build a custom serving container using the CPR model server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "480a1d88ecdb"
   },
   "source": [
    "Now that the model and processor has been trained and saved, it's time to build the custom serving container. Typically building a serving container requires writing model server code. However, with the Custom Prediction Routine feature, Vertex AI Prediction provides a model server that can be used out of the box.\n",
    "\n",
    "A custom serving container contains the following 3 pieces of code:\n",
    "1. [Model Server](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/model_server.py)\n",
    "    * HTTP server that hosts the model.\n",
    "    * Responsible for setting up routes/ports/etc.\n",
    "1. [Request Handler](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/handler.py)\n",
    "    * Responsible for webserver aspects of handling a request, such as deserializing the request body, serializing the response, setting response headers, etc.\n",
    "    * In this example, we will use the default Handler, `google.cloud.aiplatform.prediction.handler.PredictionHandler` provided in the SDK.\n",
    "1. [Predictor](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/predictor.py)\n",
    "    * Responsible for the ML logic for processing a prediction request.\n",
    "\n",
    "Each of these three pieces can be customized based on the requirements of the custom container. In this example, we will only be implementing the `Predictor`.\n",
    "\n",
    "\n",
    "A [`Predictor`](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/predictor.py) must implement the following interface:\n",
    "\n",
    "```\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Predictor(ABC):\n",
    "    \"\"\"Interface of the Predictor class for Custom Prediction Routines.\n",
    "    The Predictor is responsible for the ML logic for processing a prediction request.\n",
    "    Specifically, the Predictor must define:\n",
    "    (1) How to load all model artifacts used during prediction into memory.\n",
    "    (2) The logic that should be executed at predict time.\n",
    "    When using the default PredictionHandler, the Predictor will be invoked as follows:\n",
    "      predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, artifacts_uri: str) -> None:\n",
    "        \"\"\"Loads the model artifact.\n",
    "        Args:\n",
    "            artifacts_uri (str):\n",
    "                Required. The value of the environment variable AIP_STORAGE_URI.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, prediction_input: Any) -> Any:\n",
    "        \"\"\"Preprocesses the prediction input before doing the prediction.\n",
    "        Args:\n",
    "            prediction_input (Any):\n",
    "                Required. The prediction input that needs to be preprocessed.\n",
    "        Returns:\n",
    "            The preprocessed prediction input.\n",
    "        \"\"\"\n",
    "        return prediction_input\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, instances: Any) -> Any:\n",
    "        \"\"\"Performs prediction.\n",
    "        Args:\n",
    "            instances (Any):\n",
    "                Required. The instance(s) used for performing prediction.\n",
    "        Returns:\n",
    "            Prediction results.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def postprocess(self, prediction_results: Any) -> Any:\n",
    "        \"\"\"Postprocesses the prediction results.\n",
    "        Args:\n",
    "            prediction_results (Any):\n",
    "                Required. The prediction results.\n",
    "        Returns:\n",
    "            The postprocessed prediction results.\n",
    "        \"\"\"\n",
    "        return prediction_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KODNAqK5k8R"
   },
   "source": [
    "First, implement a custom `Predictor` that loads in both the preprocesor and the model. The preprocessor and the model will then be used at `predict` time.\n",
    "\n",
    "Custom Prediction Routine supports a way to run the containers locally for testing your images. You can pass either a GCS path or a local path while testing your images locally.\n",
    "- You need to set up the credentials if you pass a GCS path. \n",
    "- You need to support loading your models remotely and locally in your `Predictor` if you want to testing by passing a local path.\n",
    "\n",
    "Vertex SDK provides a [function `download_model_artifacts`](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/utils/prediction_utils.py) to help you download model artifacts from either GCS paths or local paths. See the example in the `load` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fkh8DT7CCgQO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serving_app/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SERVING_APP_DIR/predictor.py\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "\n",
    "class CustomPyTorchPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "    \n",
    "    def load(self, artifacts_uri: str):\n",
    "        \"\"\"Loads the model artifacts.\"\"\"\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "\n",
    "        self._model = torch.load(\"model.pt\")\n",
    "\n",
    "    def preprocess(self, prediction_input: Dict) -> torch.Tensor:\n",
    "        instances = prediction_input[\"instances\"]\n",
    "        data = pd.DataFrame(instances).values\n",
    "        return torch.Tensor(data)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, instances: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs prediction.\"\"\"\n",
    "        outputs = self._model(instances)\n",
    "        _ , predicted = torch.max(outputs, 1)\n",
    "        return predicted\n",
    "\n",
    "    def postprocess(self, prediction_results: torch.Tensor) -> Dict:\n",
    "        return {\"predictions\": [self._class_names[class_num] for class_num in prediction_results]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waA_Jl5Q6E0W"
   },
   "source": [
    "To build a custom container, we also need to write an entrypoint of the image that starts the model server. However, with the Custom Prediction Routine feature, you don't need to write the entrypoint anymore. Vertex SDK will populate the entrypoint with the custom predictor you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ddb6d57ceb"
   },
   "source": [
    "Write the dependencies to the source directory which will be installed in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We write requirements - note we will be creating our train image in our pwd, the custom predictor will be in SERVING_APP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "747f59abb3a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "fastapi\n",
    "uvicorn\n",
    "pandas\n",
    "torch\n",
    "google-cloud-storage\n",
    "google-cloud-aiplatform[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a94b4589fb20"
   },
   "outputs": [],
   "source": [
    "!cp requirements.txt $SERVING_APP_DIR/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP Endpoint Boilerplate Predictor `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serving_app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SERVING_APP_DIR/main.py\n",
    "\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "from google.cloud import storage\n",
    "from predictor import Predictor\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "predictor_instance = Predictor()\n",
    "loaded_predictor = predictor_instance.load(artifacts_uri = os.environ['AIP_STORAGE_URI'])\n",
    "\n",
    "@app.get(os.environ['AIP_HEALTH_ROUTE'], status_code=200)\n",
    "def health():\n",
    "    return {}\n",
    "\n",
    "\n",
    "@app.post(os.environ['AIP_PREDICT_ROUTE'])\n",
    "async def predict(request: Request):\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    outputs = loaded_predictor.predict(instances)\n",
    "\n",
    "    return {\"predictions\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serving_app/prestart.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SERVING_APP_DIR/prestart.sh\n",
    "#!/bin/bash\n",
    "export PORT=$AIP_HTTP_PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serving_app'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVING_APP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make it a package\n",
    "!touch src_dir_pytorch/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serving_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SERVING_APP_DIR/Dockerfile\n",
    "\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest\n",
    "WORKDIR .\n",
    "\n",
    "COPY requirements.txt /requirements.txt\n",
    "RUN pip install -r /requirements.txt\n",
    "\n",
    "COPY . /app\n",
    "EXPOSE 80\n",
    "    \n",
    "CMD [\"sh\", \"-c\", \"uvicorn main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the remote serving image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:c35981d51458e93ed290d78d23772d8b542aeadf6772970034c06d1bb76c95ef\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $SERVING_IMAGE_URI $SERVING_APP_DIR/. -q\n",
    "# !gcloud builds submit -t $REMOTE_IMAGE_NAME ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push the serving image to the artifact repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"docker push\" requires exactly 1 argument.\n",
      "See 'docker push --help'.\n",
      "\n",
      "Usage:  docker push [OPTIONS] NAME[:TAG]\n",
      "\n",
      "Push an image or a repository to a registry\n"
     ]
    }
   ],
   "source": [
    "! docker push $REMOTE_IMAGE_NAME -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dockerfile for our custom training container\n",
    "\n",
    "The [Dockerfile](https://docs.docker.com/engine/reference/builder/) specifies how to build our custom container image.\n",
    "\n",
    "This Dockerfile specifies that we want to:\n",
    "1. Use Vertex AI [prebuilt container for custom training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) as a base image.\n",
    "2. Install the required dependencied specified in our requirements.txt file.\n",
    "3. Copy our custom training script to the container image.\n",
    "4. Run our custom training script when the container starts up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copies the trainer code to the Docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our custom training image\n",
    "\n",
    "The steps required to build our image are:\n",
    "\n",
    "1. Change directory to our application directory.\n",
    "2. Build Docker image.\n",
    "3. Push the image to our Google Artifact Registry.\n",
    "4. Change directory back to our parent application directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:db156c342b4d93441a79f8e276ef28e0776c7f0051684ea25757c0b5235fa95c\n"
     ]
    }
   ],
   "source": [
    "! docker build ./ -t $TRAIN_IMAGE_URI --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push the train image to the artifact repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1-docker.pkg.dev/wortz-project-352116/custom-container-prediction-sdk/torch-cpr-train-example-train:latest\n"
     ]
    }
   ],
   "source": [
    "! docker push $TRAIN_IMAGE_URI -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now training and serving images are available on the artifact repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline for training and endpoint deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import libs\n",
    "# Kubeflow Pipelines (KFP)\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import component, Input, Output, Artifact\n",
    "\n",
    "# Google Cloud Pipeline Components (GCPC)\n",
    "from google_cloud_pipeline_components.v1 import dataset, custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "VERSION = \"v1\"\n",
    "PIPELINE_NAME = f\"{VERSION}-custom-prediciton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_ARGS = TRAINING_ARGS=[\n",
    "        \"--project_id\",\n",
    "        PROJECT_ID,\n",
    "        \"--model_bucket\",\n",
    "        BUCKET_NAME,\n",
    "        \"--model_blob\",\n",
    "        MODEL_BLOB\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE_URI,\n",
    "            \"args\": TRAINING_ARGS\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"custom_iris_pytorch\"\n",
    "ENDPOINT_NAME = f\"{MODEL_NAME}_endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME\n",
    "                  , description=\"MLOps pipeline for pytorch with cpr models\"\n",
    "                  , pipeline_root=PIPELINE_ROOT_PATH)\n",
    "def pipeline(\n",
    "    bucket_name: str = BUCKET_URI,\n",
    "    display_name: str = PIPELINE_NAME,\n",
    "    model_path: str = MODEL_URI,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    model_name: str = MODEL_NAME,\n",
    "    location: str = REGION,\n",
    "    worker_pool_specs: list = WORKER_POOL_SPEC,\n",
    "    serving_image_uri: str = SERVING_IMAGE_URI,\n",
    "    endpoint_name: str = ENDPOINT_NAME\n",
    "):\n",
    "\n",
    "    # Train model\n",
    "    model_training_op = custom_job.CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        display_name=\"train-mlops-model\",\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    importer_op = dsl.importer(\n",
    "        artifact_uri=model_path,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": serving_image_uri,\n",
    "            },\n",
    "        },\n",
    "    ).after(model_training_op)\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=model_name,\n",
    "        unmanaged_container_model=importer_op.outputs[\"artifact\"],\n",
    "    ).after(importer_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=endpoint_name,\n",
    "    ).after(model_upload_op)\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    ).after(endpoint_create_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Initialize the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compile the pipeline\n",
    "\n",
    "# ! rm -f custom_container_pipeline_spec.json\n",
    "\n",
    "\n",
    "! rm -f $PIPELINE_JSON_SPEC_LOCAL\n",
    "\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=PIPELINE_JSON_SPEC_LOCAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINES_FILEPATH: gs://pytorch-example-jsw/pipeline_root/pipeline_spec.json\n"
     ]
    }
   ],
   "source": [
    "PIPELINES_FILEPATH = f'{PIPELINE_ROOT_PATH}/pipeline_spec.json'\n",
    "print(\"PIPELINES_FILEPATH:\", PIPELINES_FILEPATH)\n",
    "\n",
    "!gsutil -q cp $PIPELINE_JSON_SPEC_LOCAL $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/679926387543/locations/us-central1/pipelineJobs/v1-custom-prediciton-20231012223552\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/679926387543/locations/us-central1/pipelineJobs/v1-custom-prediciton-20231012223552')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/v1-custom-prediciton-20231012223552?project=679926387543\n",
      "Associating projects/679926387543/locations/us-central1/pipelineJobs/v1-custom-prediciton-20231012223552 to Experiment: custom-pipe-pytorch-20231012221837\n"
     ]
    }
   ],
   "source": [
    "pipeline = aiplatform.PipelineJob(display_name=PIPELINE_NAME,\n",
    "                                  template_path=PIPELINES_FILEPATH,\n",
    "                                  pipeline_root=f'{PIPELINE_ROOT_PATH}',\n",
    "                                  enable_caching=True\n",
    "                                 )\n",
    "\n",
    "pipeline.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6883e7b07143"
   },
   "source": [
    "## Send predictions\n",
    "\n",
    "### Using Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d69ed411c2d3"
   },
   "outputs": [],
   "source": [
    "endpoint.predict(instances=[[6.7, 3.1, 4.7, 1.5], [4.6, 3.1, 1.5, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "370d22f53427"
   },
   "source": [
    "### Using REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba55bc560d58"
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = endpoint.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95c562b4e98b"
   },
   "outputs": [],
   "source": [
    "! curl \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d @instances.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa71174a7dd0"
   },
   "source": [
    "### Using gcloud CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23b8e807b02c"
   },
   "outputs": [],
   "source": [
    "!gcloud ai endpoints predict $ENDPOINT_ID \\\n",
    "  --region=$REGION \\\n",
    "  --json-request=instances.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model resource\n",
    "model.delete()\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "!gcloud artifacts docker images delete \\\n",
    "    --quiet \\\n",
    "    --delete-tags \\\n",
    "    {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SDK_Pytorch_Custom_Predict.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m110"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
